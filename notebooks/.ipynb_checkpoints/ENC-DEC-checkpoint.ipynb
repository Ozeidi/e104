{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "#Visualisation\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "%matplotlib notebook\n",
    "# from pylab import rcParams\n",
    "# from matplotlib import gridspec\n",
    "# import seaborn as sns\n",
    "# from ggplot import *\n",
    "# import plotly.plotly as py\n",
    "# #Formatted Print \n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "#NILMTK\n",
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "from nilmtk.datastore import HDFDataStore\n",
    "from nilmtk.electric import align_two_meters\n",
    "\n",
    "\n",
    "\n",
    "allowed_key_names = ['fridge','microwave','dish_washer','kettle','washing_machine']\n",
    "WINDOW_PER_BUILDING = {\n",
    "    1: (\"2013-04-12\", \"2014-12-15\"),\n",
    "    2: (\"2013-05-22\", \"2013-10-03 06:16:00\"),\n",
    "    3: (\"2013-02-27\", \"2013-03-27 06:15:05\"),\n",
    "    4: (\"2013-03-09\", \"2013-04-24 06:15:14\"),\n",
    "    5: (\"2014-06-29\", \"2014-09-01\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data (key_name,model_name, end_e):\n",
    "\n",
    "    \n",
    "    sp =6\n",
    "    # =======  Open configuration file  ===============\n",
    "    appliances = json.load(open(\"../conf/apps.json\",'r'))\n",
    "    \n",
    "    nilmtk_key= appliances[key_name]['key']\n",
    "    \n",
    "    v = appliances[key_name]\n",
    "    data_path='/projects/da33/ozeidi/Project/data/UKDALE/ukdale.h5'\n",
    "    #training_set = DataSet(data_path)\n",
    "    validation_set = DataSet(data_path)\n",
    "\n",
    "    #disag_filename =  '{}/{}_disag-out_{}epochs.h5'.format(save_path,nilmtk_key,end_e) # The filename of the resulting datastore\n",
    "    output = DataSet(disag_filename)\n",
    "\n",
    "    val_builidng = v['validation_buildings'][0]\n",
    "\n",
    "\n",
    "    validation_set.set_window(start=WINDOW_PER_BUILDING[val_builidng][0],end=WINDOW_PER_BUILDING[val_builidng][1])\n",
    "    # This makes a list of references to the NILMTK main meters and submeter\n",
    "    # no data is generated from them at this point\n",
    "    mains = validation_set.buildings[val_builidng].elec.mains().power_series_all_data(sample_period=sp)\n",
    "    meter = validation_set.buildings[val_builidng].elec.submeters()[nilmtk_key].power_series_all_data(sample_period=sp)\n",
    "    return mains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:766: DeprecationWarning: builtin type EagerTensor has no __module__ attribute\n",
      "  EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\n",
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:4046: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/osal10/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/usr/local/anaconda/5.0.1-Python3.6-gcc5/envs/tensorflow-1.4-py3.6/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:509: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from  tensorflow.python.keras.models import Model\n",
    "from  tensorflow.python.keras.layers import Input, LSTM, Dense\n",
    "from  tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "latent_dim = 50 # LSTM hidden units\n",
    "dropout = .20 \n",
    "\n",
    "# Define an input series and encode it with an LSTM. \n",
    "encoder_inputs = Input(shape=(None, 1)) \n",
    "encoder = LSTM(latent_dim, dropout=dropout, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the final states. These represent the \"context\"\n",
    "# vector that we use as the basis for decoding.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# This is where teacher forcing inputs are fed in.\n",
    "decoder_inputs = Input(shape=(None, 1)) \n",
    "\n",
    "# We set up our decoder using `encoder_states` as initial state.  \n",
    "# We return full output sequences and return internal states as well. \n",
    "# We don't use the return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(1) # 1 continuous output at each timestep\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, None, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 50), (None, 5 10400       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, None, 50), (N 10400       input_2[0][0]                    \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, None, 1)       51          lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 20,851\n",
      "Trainable params: 20,851\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/projects/da33/ozeidi/Project/data/UKDALE/ukdale.h5'\n",
    "#training_set = DataSet(data_path)\n",
    "validation_set = DataSet(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_n_samples = 20000\n",
    "batch_size = 2**11\n",
    "epochs = 100\n",
    "\n",
    "# sample of series from train_enc_start to train_enc_end  \n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, \n",
    "                                           train_enc_start, train_enc_end)[:first_n_samples]\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "# sample of series from train_pred_start to train_pred_end \n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, \n",
    "                                            train_pred_start, train_pred_end)[:first_n_samples]\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n",
    "\n",
    "# lagged target series for teacher forcing\n",
    "decoder_input_data = np.zeros(decoder_target_data.shape)\n",
    "decoder_input_data[:,1:,0] = decoder_target_data[:,:-1,0]\n",
    "decoder_input_data[:,0,0] = encoder_input_data[:,-1,0]\n",
    "\n",
    "model.compile(Adam(), loss='mean_absolute_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
